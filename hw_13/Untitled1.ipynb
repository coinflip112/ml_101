{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from diacritization import recodex_predict\n",
    "from diacritization import Dataset\n",
    "import numpy as np\n",
    "import lzma\n",
    "import pickle\n",
    "from diacritization import flatten\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset()\n",
    "with lzma.open(\"diacritization.model\", \"rb\") as model_file:\n",
    "    nn = pickle.load(model_file)\n",
    "\n",
    "with lzma.open(\"onehot.encoder\", \"rb\") as model_file:\n",
    "    ohe = pickle.load(model_file)\n",
    "\n",
    "with lzma.open(\"label.encoder\", \"rb\") as model_file:\n",
    "    le = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(s, n):\n",
    "    return zip(*[s[i:] for i in range(n)])\n",
    "\n",
    "input_len = 13\n",
    "features = np.array([\n",
    "        np.array(\n",
    "            list(\n",
    "                find_ngrams(\n",
    "                    \"\".join([\"#\" for i in range(int((input_len - 1) / 2))])\n",
    "                    + sentence.lower()\n",
    "                    + \"\".join([\"#\" for i in range(int((input_len - 1) / 2))]),\n",
    "                    input_len,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        for sentence in data.data.translate(Dataset.DIA_TO_NODIA).split(\"\\n\")[:-1]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ohe.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn.predict(features[:200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = le.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563360"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.data.strip(\"\\n\").strip(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553990"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def capitalize_word(accented_word, capitalized_word):\n",
    "    try:\n",
    "        final_capitalized_word = []\n",
    "        for accented_character, capitalized_character in zip(\n",
    "            accented_word, capitalized_word\n",
    "        ):\n",
    "            if capitalized_character.isupper():\n",
    "                final_capitalized_word.append(accented_character.upper())\n",
    "            else:\n",
    "                final_capitalized_word.append(accented_character)\n",
    "        return \"\".join(final_capitalized_word)\n",
    "    except:\n",
    "        return capitalized_word\n",
    "\n",
    "def sentence_predict(sentence_sliding_window, sentence_orig, nn, ohe, le):\n",
    "    try:\n",
    "        ohe_sentence = ohe.transform(sentence_sliding_window)\n",
    "        predictions = nn.predict(ohe_sentence)\n",
    "        characters = le.inverse_transform(predictions)\n",
    "        if len(characters) != len(sentence_orig):\n",
    "            return sentence_orig\n",
    "        sentence_predicted = \"\".join(\n",
    "            np.where(\n",
    "                (np.array(list(characters)) == \" \")\n",
    "                & (np.array(list(sentence_orig)) != \" \"),\n",
    "                np.array(list(sentence_orig)),\n",
    "                np.array(list(characters)),\n",
    "            )\n",
    "        )\n",
    "        words_predicted = sentence_predicted.split(\" \")\n",
    "        words_orig = sentence_orig.split(\" \")\n",
    "        words_capitalized = [\n",
    "            capitalize_word(accented_word, orig_word)\n",
    "            for accented_word, orig_word in zip(words_predicted, words_orig)\n",
    "        ]\n",
    "        return \" \".join(words_capitalized)\n",
    "    except:\n",
    "        return sentence_orig\n",
    "\n",
    "predicted_sentences = [\n",
    "    sentence_predict(transformed_sentence, orig_sentence, nn, ohe, le)\n",
    "    for transformed_sentence, orig_sentence in zip(features, sentences)\n",
    "]\n",
    "\n",
    "predictions = \"\\n\".join(predicted_sentences)\n",
    "return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gold.txt\", \"w\") as text_file:\n",
    "    text_file.write(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pred.txt\", \"w\") as text_file:\n",
    "    text_file.write(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
