{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "import lzma\n",
    "import pickle\n",
    "import os\n",
    "# os.chdir(\"hw_14\")\n",
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "import unicodedata\n",
    "\n",
    "from collections import namedtuple\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize(\"NFKD\", input_str)\n",
    "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from diacritization import recodex_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "\n",
    "\n",
    "class Dictionary:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name=\"fiction-dictionary.txt\",\n",
    "        url=\"https://ufal.mff.cuni.cz/~straka/courses/npfl129/1920/datasets/\",\n",
    "    ):\n",
    "        if not os.path.exists(name):\n",
    "            print(\"Downloading {}...\".format(name), file=sys.stderr)\n",
    "            urllib.request.urlretrieve(url + name, filename=name)\n",
    "            urllib.request.urlretrieve(\n",
    "                url + name.replace(\".txt\", \".LICENSE\"),\n",
    "                filename=name.replace(\".txt\", \".LICENSE\"),\n",
    "            )\n",
    "\n",
    "        # Load the dictionary to `variants`\n",
    "        self.variants = {}\n",
    "        with open(name, \"r\", encoding=\"utf-8\") as dictionary_file:\n",
    "            for line in dictionary_file:\n",
    "                nodia_word, *variants = line.rstrip(\"\\n\").split()\n",
    "                self.variants[nodia_word] = variants\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    LETTERS_NODIA = \"acdeeinorstuuyz\"\n",
    "    LETTERS_DIA = \"áčďéěíňóřšťúůýž\"\n",
    "\n",
    "    # A translation table usable with `str.translate` to rewrite characters with dia to the ones without them.\n",
    "    DIA_TO_NODIA = str.maketrans(\n",
    "        LETTERS_DIA + LETTERS_DIA.upper(), LETTERS_NODIA + LETTERS_NODIA.upper()\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name=\"fiction-train.txt\",\n",
    "        url=\"https://ufal.mff.cuni.cz/~straka/courses/npfl129/1920/datasets/\",\n",
    "    ):\n",
    "        if not os.path.exists(name):\n",
    "            print(\"Downloading dataset {}...\".format(name), file=sys.stderr)\n",
    "            urllib.request.urlretrieve(url + name, filename=name)\n",
    "            urllib.request.urlretrieve(\n",
    "                url + name.replace(\".txt\", \".LICENSE\"),\n",
    "                filename=name.replace(\".txt\", \".LICENSE\"),\n",
    "            )\n",
    "\n",
    "        # Load the dataset and split it into `data` and `target`.\n",
    "        with open(name, \"r\", encoding=\"utf-8\") as dataset_file:\n",
    "            self.data = dataset_file.read()\n",
    "\n",
    "\n",
    "train = Dataset().data\n",
    "\n",
    "characters = list(np.unique(list(remove_accents(train.lower())))[1:]) + [\"#\"]\n",
    "\n",
    "sentences_train = train.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "diac_source_target_combo = namedtuple(\"diac_combo\", [\"source\", \"target\"])\n",
    "diacritization_mapping = {\n",
    "    diac_source_target_combo(\"a\", \"á\"): \"acute\",\n",
    "    diac_source_target_combo(\"c\", \"č\"): \"caron\",\n",
    "    diac_source_target_combo(\"d\", \"ď\"): \"caron\",\n",
    "    diac_source_target_combo(\"e\", \"ě\"): \"caron\",\n",
    "    diac_source_target_combo(\"i\", \"í\"): \"acute\",\n",
    "    diac_source_target_combo(\"n\", \"ň\"): \"caron\",\n",
    "    diac_source_target_combo(\"o\", \"ó\"): \"acute\",\n",
    "    diac_source_target_combo(\"r\", \"ř\"): \"caron\",\n",
    "    diac_source_target_combo(\"s\", \"š\"): \"caron\",\n",
    "    diac_source_target_combo(\"t\", \"ť\"): \"caron\",\n",
    "    diac_source_target_combo(\"u\", \"ů\"): \"ring\",\n",
    "    diac_source_target_combo(\"y\", \"ý\"): \"acute\",\n",
    "    diac_source_target_combo(\"z\", \"ž\"): \"caron\",\n",
    "}\n",
    "diacritization_transform = {\n",
    "    \"a\": \"á\",\n",
    "    \"c\": \"č\",\n",
    "    \"d\": \"ď\",\n",
    "    \"e\": \"ě\",\n",
    "    \"i\": \"í\",\n",
    "    \"n\": \"ň\",\n",
    "    \"o\": \"ó\",\n",
    "    \"r\": \"ř\",\n",
    "    \"s\": \"š\",\n",
    "    \"t\": \"ť\",\n",
    "    \"u\": \"ů\",\n",
    "    \"y\": \"ý\",\n",
    "    \"z\": \"ž\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = 21\n",
    "def find_ngrams(s, n):\n",
    "    return zip(*[s[i:] for i in range(n)])\n",
    "\n",
    "\n",
    "def get_simple_labels(\n",
    "    feature,\n",
    "    target,\n",
    "    letter_position=input_len // 2,\n",
    "    diacritization_mapping=diacritization_mapping,\n",
    "):\n",
    "    feature_letter = feature[letter_position]\n",
    "    source_target_mapping = diac_source_target_combo(\n",
    "        source=feature_letter, target=target\n",
    "    )\n",
    "    if source_target_mapping in diacritization_mapping:\n",
    "        return diacritization_mapping[source_target_mapping]\n",
    "    else:\n",
    "        return \"no_change\"\n",
    "\n",
    "def get_even_simpler_labels(\n",
    "    feature,\n",
    "    target,\n",
    "    letter_position=input_len // 2,\n",
    "    diacritization_mapping=diacritization_mapping,\n",
    "):\n",
    "    feature_letter = feature[letter_position]\n",
    "    source_target_mapping = diac_source_target_combo(\n",
    "        source=feature_letter, target=target\n",
    "    )\n",
    "    if source_target_mapping in diacritization_mapping:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "features = np.array(\n",
    "    flatten(\n",
    "        [\n",
    "            list(\n",
    "                find_ngrams(\n",
    "                    \"\".join([\"#\" for i in range(int((input_len - 1) / 2))])\n",
    "                    + remove_accents(sentence.lower())\n",
    "                    + \"\".join([\"#\" for i in range(int((input_len - 1) / 2))]),\n",
    "                    input_len,\n",
    "                )\n",
    "            )\n",
    "            for sentence in sentences_train\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "targets = np.array(\n",
    "    [\n",
    "        feature[int((input_len - 1) / 2)]\n",
    "        for feature in flatten(\n",
    "            [\n",
    "                list(\n",
    "                    find_ngrams(\n",
    "                        \"\".join([\"#\" for i in range(int((input_len - 1) / 2))])\n",
    "                        + sentence.lower()\n",
    "                        + \"\".join([\"#\" for i in range(int((input_len - 1) / 2))]),\n",
    "                        input_len,\n",
    "                    )\n",
    "                )\n",
    "                for sentence in sentences_train\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "targets = np.array(\n",
    "    [get_even_simpler_labels(feature, target) for feature, target in zip(features, targets)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with lzma.open(\"diacritization.model\", \"rb\") as model_file:\n",
    "    nn = pickle.load(model_file)\n",
    "\n",
    "with lzma.open(\"onehot.encoder\", \"rb\") as model_file:\n",
    "    ohe = pickle.load(model_file)\n",
    "\n",
    "with lzma.open(\"label.encoder\", \"rb\") as model_file:\n",
    "    le = pickle.load(model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'a'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.translate(\"a\",Dataset.DIA_TO_NODIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "no_change    0.901946\ncaron        0.052862\nacute        0.042346\nring         0.002847\ndtype: float64"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(targets).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Měla na ruce nejkrásnější náramek , jaký jsem kdy viděl - secesní víla se na něm propletala mezi brilianty a smaragdy .\\nTenhle ňaramek mel cenu luxusňiho auta , jeňze jeho křasa byla jěsťe veťši , byl to jeden z ťech predmeťu , na ktere clovek pohledne a vzďava chvalu Stvóriteli , kteřy dal lidem takove ňaďaňi .\\nStára dama si vsimla , ze se na naramek dívam .\\n\" Libi sě vam ? \"\\n\" Hežci sperk jsem jeste něvidel , \" rekl jsem .\\n\" Snaď sí rikate - ta bůde mit hoďne pekňych šperku , \" usmala se na me .\\n\" Ale tohle je jediný cenny klenot , co mam . \"\\nOdepjala naramek a polozila hó na štul .\\nVsimla jsěm si , ze pokoj je zaplneny kvetiňami , zrejme k tehle ďivce kazdy ňejake prinesl - a podle ťoho k ni musela chodit spoůsta lidi .\\nDaly jsme še do reci .\\nJěstli si predstávujete , ze mluvila nejakym kazátelskym tonem nebo vemlouvavym hlasem , kdepak .\\nMluvila , jako kdyby bylá mou kamaradkou z diskoteky , šmala se kazdou chvili - ťó býlo zvlastni , sťale vyhledávala nějaky vesely rys ňa tom , o cem jsme mluvíly , í kdyz to vubeč vesěle zdanlivě nebýlo .\\nTim smičhem ale dodavala každemů jevu nejaky šnesitelnejsí smysl .\\nKdyz jšem si k babiccě v nedeli věcěr pro ni prisla , myšlela jšem , že omdlim .\\nŮdelala mi jiny limec !\\nPrece jsem jí podrobne výsvětlíla , jáky ma být vystrih u blůzy , jaky limecek , a ona mi ůdela neco uplne jineho !\\nV sestnáctí jsem byla pekny rozmazlenec a něbyla jsem zvykla , áby mi babicka nekďy nevyhovela .\\nKřičela jsem , vyvadela á nebyla k utisení .\\nŇa ťu oslavů mel prijit on , sěštnactileťy spoluzak , a zdálo se mi , '"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "1551it [00:22, 58.86it/s]"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-d37bb07d890a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0morig_char_is_upper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_char\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0morig_char_lowered\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLETTERS_NODIA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mchar_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mpredicted_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpredicted_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_change\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml_101/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    730\u001b[0m                                        copy=True)\n\u001b[1;32m    731\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml_101/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform_new\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;34m\"\"\"New implementation assuming categorical input\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;31m# validation of X happens in _check_X called by _transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml_101/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mXi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             diff, valid_mask = _encode_check_unknown(Xi, self.categories_[i],\n\u001b[0;32m--> 116\u001b[0;31m                                                      return_mask=True)\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml_101/lib/python3.7/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36m_encode_check_unknown\u001b[0;34m(values, uniques, return_mask)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \"\"\"\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0muniques_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0muniques_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prediction = \"\"\n",
    "for orig_char, char_features in tqdm.tqdm(zip(str.translate(train,Dataset.DIA_TO_NODIA),features)):\n",
    "    orig_char_lowered = orig_char.lower()\n",
    "    orig_char_is_upper = orig_char.isupper()\n",
    "    if orig_char_lowered in Dataset.LETTERS_NODIA:\n",
    "        char_features = ohe.transform(char_features.reshape(1,-1))\n",
    "        predicted_change = nn.predict(char_features)\n",
    "        predicted_change = le.inverse_transform(predicted_change)\n",
    "        if predicted_change != \"no_change\":\n",
    "            prediced_char = diacritization_transform[orig_char_lowered]\n",
    "            if orig_char_is_upper:\n",
    "                prediced_char = prediced_char.upper()\n",
    "            prediction += prediced_char\n",
    "        else:\n",
    "            prediction += orig_char\n",
    "    else:\n",
    "        prediction += orig_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "ohe = OneHotEncoder(\n",
    "    handle_unknown=\"ignore\", categories=[sorted(characters) for i in range(input_len)]\n",
    ")\n",
    "features = ohe.fit_transform(features)\n",
    "\n",
    "nn = MLPClassifier(\n",
    "    hidden_layer_sizes=(\n",
    "        400,\n",
    "        375,\n",
    "        350,\n",
    "        325,\n",
    "        300,\n",
    "        275,\n",
    "        250,\n",
    "        225,\n",
    "        200,\n",
    "        175,\n",
    "        150,\n",
    "        125,\n",
    "        100,\n",
    "        75,\n",
    "        50,\n",
    "        30,\n",
    "    ),\n",
    "    verbose=4,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.05,\n",
    "    max_iter=400,\n",
    "    n_iter_no_change=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with lzma.open(\"diacritization.model\", \"wb\") as model_file:\n",
    "    pickle.dump(nn, model_file)\n",
    "\n",
    "with lzma.open(\"onehot.encoder\", \"wb\") as model_file:\n",
    "    pickle.dump(ohe, model_file)\n",
    "\n",
    "with lzma.open(\"label.encoder\", \"wb\") as model_file:\n",
    "    pickle.dump(le, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = data.split(\"\\n\")\n",
    "\n",
    "\n",
    "def find_ngrams(s, n):\n",
    "    return zip(*[s[i:] for i in range(n)])\n",
    "\n",
    "\n",
    "input_len = 13\n",
    "features = np.array(\n",
    "    [\n",
    "        list(\n",
    "            find_ngrams(\n",
    "                \"\".join([\"#\" for i in range(int((input_len - 1) / 2))])\n",
    "                + remove_accents(sentence.lower())\n",
    "                + \"\".join([\"#\" for i in range(int((input_len - 1) / 2))]),\n",
    "                input_len,\n",
    "            )\n",
    "        )\n",
    "        for sentence in sentences\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def capitalize_word(accented_word, capitalized_word):\n",
    "    final_capitalized_word = []\n",
    "    if len(accented_word) != len(capitalized_word):\n",
    "        return capitalized_word\n",
    "    for accented_character, capitalized_character in zip(\n",
    "        accented_word, capitalized_word\n",
    "    ):\n",
    "        if capitalized_character.isupper():\n",
    "            final_capitalized_word.append(accented_character.upper())\n",
    "        else:\n",
    "            final_capitalized_word.append(accented_character)\n",
    "    return \"\".join(final_capitalized_word)\n",
    "\n",
    "\n",
    "def sentence_predict(sentence_sliding_window, sentence_orig, nn, ohe, le):\n",
    "    if len(sentence_orig) == 0:\n",
    "        return \"\"\n",
    "    ohe_sentence = ohe.transform(sentence_sliding_window)\n",
    "    predictions = nn.predict(ohe_sentence)\n",
    "    characters = le.inverse_transform(predictions)\n",
    "    sentence_predicted = \"\".join(characters)\n",
    "    words_predicted = sentence_predicted.split(\" \")\n",
    "    words_orig = sentence_orig.split(\" \")\n",
    "    words_capitalized = [\n",
    "        capitalize_word(accented_word, orig_word)\n",
    "        for accented_word, orig_word in zip(words_predicted, words_orig)\n",
    "    ]\n",
    "    return \" \".join(words_capitalized)\n",
    "\n",
    "\n",
    "predicted_sentences = [\n",
    "    sentence_predict(transformed_sentence, orig_sentence, nn, ohe, le)\n",
    "    for transformed_sentence, orig_sentence in zip(features, sentences)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}